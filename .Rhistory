# RANDOMLY select EXISTING GI run from which to take the fitting parameters
existing_run <- sample(py_files_G_ps$existing,1)
existing_file <- paste('PS',ps,'_G_E000_R',existing_run,'.py',sep='')
unzip(paste(ps,'_G_py.zip',sep=''), existing_file)
x <- readLines(existing_file)
params_GI <- get_generalized_immunity(ps, existing_run)
# Get the seed from the corresponding selection scenario
unzip(paste(ps,'_S_py.zip',sep=''), paste('PS',ps,'_S_E000_R',missing_run,'.py',sep=''))
seed <- get_random_seed(PS = ps, scenario = 'S', experiment = '000', run_range = missing_run)
unlink(paste('PS',ps,'_S_E000_R',missing_run,'.py',sep=''))
# Use the parameters to generate the file
design_subset <- subset(design, PS %in% ps & scenario=='G' & exp=='000')
generate_files(row_range = 1, random_seed = seed, run_range = missing_run, experimental_design = design_subset, params_GI = params_GI)
# Remove the existing file
unlink(existing_file)
}
# Get the seeds used for the missing runs to generate the rest of the experiments
seeds <- get_random_seed(PS = ps, scenario = 'G', run_range = py_files_G_ps$missing)
seeds
# Generate control experimets
design_subset <- subset(design, PS %in% ps & scenario=='G' & exp=='001')
generate_files(row_range = 1, run_range = py_files_G_ps$missing, experimental_design = design_subset, random_seed = seeds)
# Generate IRS experiments
design_irs <- create_intervention_scheme_IRS(PS_benchmark = ps, scenario_benchmark = work_scenario, IRS_START_TIMES = '29160', immigration_range=c(0), length_range=c(720,1800,3600), coverage_range=0.9, poolsize_bounce = 'False', write_to_file = F, design_ref=design)
generate_files(row_range = 1:nrow(design_irs), run_range = py_files_G_ps$missing,
random_seed = seeds,
experimental_design=design_irs)
for (ps in ps_range){
py_files_G_ps <- py_files_G[[which(ps_range==ps)]]
# Create 000 files for all runs
for (missing_run in py_files_G_ps$missing){
# RANDOMLY select EXISTING GI run from which to take the fitting parameters
existing_run <- sample(py_files_G_ps$existing,1)
existing_file <- paste('PS',ps,'_G_E000_R',existing_run,'.py',sep='')
unzip(paste(ps,'_G_py.zip',sep=''), existing_file)
x <- readLines(existing_file)
params_GI <- get_generalized_immunity(ps, existing_run)
# Get the seed from the corresponding selection scenario
unzip(paste(ps,'_S_py.zip',sep=''), paste('PS',ps,'_S_E000_R',missing_run,'.py',sep=''))
seed <- get_random_seed(PS = ps, scenario = 'S', experiment = '000', run_range = missing_run)
unlink(paste('PS',ps,'_S_E000_R',missing_run,'.py',sep=''))
# Use the parameters to generate the file
design_subset <- subset(design, PS %in% ps & scenario=='G' & exp=='000')
generate_files(row_range = 1, random_seed = seed, run_range = missing_run, experimental_design = design_subset, params_GI = params_GI)
# Remove the existing file
unlink(existing_file)
}
# Get the seeds used for the missing runs to generate the rest of the experiments
seeds <- get_random_seed(PS = ps, scenario = 'G', run_range = py_files_G_ps$missing)
# Generate control experimets
design_subset <- subset(design, PS %in% ps & scenario=='G' & exp=='001')
generate_files(row_range = 1, run_range = py_files_G_ps$missing, experimental_design = design_subset, random_seed = seeds)
# Generate IRS experiments
design_irs <- create_intervention_scheme_IRS(PS_benchmark = ps, scenario_benchmark = work_scenario, IRS_START_TIMES = '29160', immigration_range=c(0), length_range=c(720,1800,3600), coverage_range=0.9, poolsize_bounce = 'False', write_to_file = F, design_ref=design)
generate_files(row_range = 1:nrow(design_irs), run_range = py_files_G_ps$missing,
random_seed = seeds,
experimental_design=design_irs)
# Generate sbatch files
generate_sbatch(ps = ps, scen = 'G', runs = py_files_G_ps$missing, experiment = '000', unzip_py_files = F)
generate_sbatch(ps = ps, scen = 'G', runs = py_files_G_ps$missing, experiment = '001', unzip_py_files = F)
generate_sbatch(ps = ps, scen = 'G', runs = py_files_G_ps$missing, experiment = '002', unzip_py_files = F)
generate_sbatch(ps = ps, scen = 'G', runs = py_files_G_ps$missing, experiment = '003', unzip_py_files = F)
generate_sbatch(ps = ps, scen = 'G', runs = py_files_G_ps$missing, experiment = '004', unzip_py_files = F)
}
# Create the reference experiments (checkpoint and control)
ps_range <- sprintf('%0.2d', 27:39)
exp_range <- sprintf('%0.3d', 0:4)
run_range <- 1:50
work_scenario <- 'G'
# ZIP all the PY and sbatch files
unlink('files_to_run.txt')
sink('files_to_run.txt', append = T)
# Add sbatch files
files <- list.files(path = '~/Documents/malaria_interventions_data/', pattern = 'sbatch', full.names = F)
files <- files[str_detect(string = files, paste(work_scenario,'E',sep=''))]
files <- files[str_sub(files,3,4) %in% ps_range]
files <- files[str_sub(files,7,9) %in% exp_range]
for (i in 1:length(files)){
cat(files[i]);cat('\n')
}
# Add py files
files <- list.files(path = '~/Documents/malaria_interventions_data/', pattern = '.py', full.names = F)
files <- files[str_detect(string = files, paste('_',work_scenario,sep=''))]
files <- files[str_sub(files,3,4) %in% ps_range]
files <- files[str_sub(files,9,11) %in% exp_range]
files <- files[parse_number(str_sub(files,14,15)) %in% run_range]
for (i in 1:length(files)){
cat(files[i]);cat('\n')
}
sink.reset()
# Create zip
unlink('files_to_run.zip')
system('zip files_to_run.zip -@ < files_to_run.txt')
# First run the checkpoints
paste("for i in ",paste(ps_range,collapse=' '),"; do sbatch 'PS'$i'",work_scenario,"E000.sbatch'; done;",sep='')
# Or, if checkpoints are already finished:
unlink('jobs_to_run.sh')
sink('jobs_to_run.sh')
for (ps in ps_range){
for (e in exp_range[-1]){
cat(paste('sbatch PS',ps,work_scenario,'E',e,'.sbatch',sep=''));cat('\n')
}
}
sink.reset()
setwd("~/Documents/malaria_interventions")
### USE SPARSE MATRICES FOR CUTOFF:
# x <- xtabs(~strainId+geneId, strainComposition, sparse = T)
# # This applies the overlapAlleleAdj function directly
# similarityMatrix <- tcrossprod(x>0)
# similarityMatrix <- similarityMatrix/rowSums(x>0)
# # Some info:
# print(object.size(similarityMatrix))
# dim(similarityMatrix)
# cutoffValue <- quantile(as.vector(similarityMatrix), probs = cutoffPercentile)
# print(cutoffValue)
# # hist(as.vector(similarityMatrix));abline(v=cutoffValue,col='red')
#
# # Free up memory
# rm(x)
#
# # This removes the values lower than the cutoff
# similarityMatrix <- as(similarityMatrix, "dgTMatrix")
# ind <- similarityMatrix@x < cutoffValue
# similarityMatrix@x <- similarityMatrix@x[!ind]
# similarityMatrix@i <- similarityMatrix@i[!ind]
# similarityMatrix@j <- similarityMatrix@j[!ind]
# mprint(paste('Writing sparse matrix to file: ',filenameBase,'_similarityMatrix.mtx',sep=''))
# writeMM(similarityMatrix, paste(filenameBase,'_similarityMatrix.mtx',sep=''))
library(tidyverse)
library(magrittr)
library(sqldf)
library(igraph)
# Functions ---------------------------------------------------------------
## @knitr FUNCTIONS
source('functions.R')
## @knitr END
if (length(commandArgs(trailingOnly=TRUE))==0) {
args <- c('27','S','001')
} else {
args <- commandArgs(trailingOnly=TRUE)
}
# Initialize --------------------------------------------------------------
source('functions.R')
prep.packages(c('dplyr','readr','stringr','tidyr','tibble','magrittr','purrr','sqldf'))
if (length(commandArgs(trailingOnly=TRUE))==0) {
args <- c('27','S','001')
} else {
args <- commandArgs(trailingOnly=TRUE)
}
job_ps <- as.character(args[1])
job_scenario <- as.character(args[2])
job_exp <- as.character(args[3])
job_run=1
data <- get_data(parameter_space = job_ps, scenario = job_scenario, experiment = job_exp, run = job_run)
# Initialize --------------------------------------------------------------
source('functions.R')
data <- get_data(parameter_space = job_ps, scenario = job_scenario, experiment = job_exp, run = job_run, host_age_structure = T)
source('~/Documents/malaria_interventions/functions.R', echo=TRUE)
data <- get_data(parameter_space = job_ps, scenario = job_scenario, experiment = job_exp, run = job_run, host_age_structure = T)
data$summary_general
data$sampled_infections
base_name <- paste('PS',job_ps,'_',job_scenario,'_E',job_exp,'_R',job_run,sep='')
base_name
write_csv(data$summary_general, paste(base_name,'_summary_general.csv'))
network <- createTemporalNetwork(ps = job_ps,
scenario = job_scenario,
exp = job_exp, run = job_run,
cutoff_prob = 0.9,
cutoff_value = NULL,
write_files = F,
layers_to_include = NULL,
sampled_infections = data$sampled_infections)
source('~/Documents/malaria_interventions/functions.R', echo=TRUE)
network <- createTemporalNetwork(ps = job_ps,
scenario = job_scenario,
exp = job_exp, run = job_run,
cutoff_prob = 0.9,
cutoff_value = NULL,
write_files = F,
layers_to_include = 1:20,
sampled_infections = data$sampled_infections)
createTemporalNetwork <- function(ps, scenario, exp, run, cutoff_prob=0.9, cutoff_value=NULL, write_files=F, layers_to_include=NULL, sampled_infections=NULL){
base_name <- paste('PS',ps,'_',scenario,'_E',exp,'_R',run,sep='')
if (on_Midway()){
sqlite_file <- paste('sqlite/',base_name,'.sqlite',sep='')
} else {
sqlite_file <- paste('~/Documents/malaria_interventions_data/',base_name,'.sqlite',sep='')
}
# Extract data from sqlite. variable names correspond to table names
db <- dbConnect(SQLite(), dbname = sqlite_file)
print('Getting genetic data from sqlite...')
sampled_strains <- as.tibble(dbGetQuery(db, 'SELECT id, gene_id FROM sampled_strains'))
names(sampled_strains)[1] <- c('strain_id')
sampled_alleles <- as.tibble(dbGetQuery(db, 'SELECT * FROM sampled_alleles'))
names(sampled_alleles)[3] <- c('allele_id')
sampled_strains <- full_join(sampled_strains, sampled_alleles)
sampled_strains$allele_locus <- paste(sampled_strains$allele_id,sampled_strains$locus,sep='_') # each allele in a locus is unique
dbDisconnect(db)
# Get infection data
if (is.null(sampled_infections)){
print('Getting infection data from sqlite...')
sampled_infections <- get_data(parameter_space = ps, experiment = exp, scenario = scenario, run = run)$sampled_infections
}
print('Building data set...')
# Add layer numbers. Each time point is a layer
sampled_infections$layer <- group_indices(sampled_infections, time)
sampled_infections %<>% arrange(layer,strain_id,host_id) %>%
group_by(layer,strain_id) %>%
mutate(strain_copy = row_number()) # add unique id for each strain copy within each layer. A strain copy is an instance of a strain in a particualr host
sampled_infections$strain_id_unique <- paste(sampled_infections$strain_id,sampled_infections$strain_copy,sep='_') # Create the unique strains
# Integrate the strain composition into the infections table
if (all(unique(sampled_strains$strain_id)%in%unique(sampled_infections$strain_id))==F || all(unique(sampled_infections$strain_id)%in%unique(sampled_strains$strain_id))==F) {
stop('There may be a mismatch in repertoires between the sampled_strains and sampled_infections data sets. Revise!')
}
sampled_infections %<>% select(time, layer, host_id, strain_id, strain_copy, strain_id_unique) %>%
left_join(sampled_strains)
print('Building layers...')
Layers <- list()
if (is.null(layers_to_include)){
layers_to_include <- sort(unique(sampled_infections$layer))
}
for (l in layers_to_include){
cat(paste('[',Sys.time(), '] building layer ',l,'\n',sep=''))
sampled_infections_layer <- subset(sampled_infections, layer==l) # This is MUCH faster than sampled_infections_layer <- sampled_infections %>% filter(layer==l)
Layers[[which(layers_to_include==l)]] <- build_layer(sampled_infections_layer)
}
# Get just the matrices
temporal_network <- lapply(Layers, function(x) x$similarity_matrix)
layer_summary <- do.call(rbind, lapply(Layers, function(x) x$layer_summary))
layer_summary$layer <- layers_to_include
sapply(temporal_network, nrow)
print('Applying cutoff...')
# Apply a cutoff
## Get the similarity matrix for all the repertoires and alleles
x <- xtabs(~strain_id+allele_locus, subset(sampled_strains, strain_id%in%sampled_infections$strain_id))
similarityMatrix <- overlapAlleleAdj(x)
dim(similarityMatrix)
if (is.null(cutoff_value)){
cutoff_value <- quantile(as.vector(similarityMatrix), probs = 0.9)
}
print(cutoff_value)
# hist(as.vector(similarityMatrix));abline(v=cutoff_value,col='red')
similarityMatrix[similarityMatrix<cutoff_value] <- 0
for (i in 1:length(temporal_network)){
# print(i)
x <- temporal_network[[i]]
x[x<cutoff_value] <- 0
temporal_network[[i]] <- x
}
if(write_files){
print('Writing similarity matrices to files...')
# Write similarity matrix without cutoff. This is used to plot the edge weight distributions.
write.table(similarityMatrix, paste('../',filenameBase,'_similarityMatrix_nocutoff.csv',sep=''), row.names = T, col.names = T, sep=',')
# write.table(similarityMatrix, paste(filenameBase,'_similarityMatrix.csv',sep=''), row.names = T, col.names = T, sep=',')
# write.table(similarityMatrix, paste('../',filenameBase,'_similarityMatrix.csv',sep=''), row.names = T, col.names = T, sep=',')
}
print('Done!')
return(list(temporal_network=temporal_network, similarityMatrix=similarityMatrix, cutoff_value=cutoff_value, layer_summary=layer_summary, base_name = base_name, ps=ps, scenario=scenario, experiment=exp, run=run))
}
network <- createTemporalNetwork(ps = job_ps,
scenario = job_scenario,
exp = job_exp, run = job_run,
cutoff_prob = 0.9,
cutoff_value = NULL,
write_files = F,
layers_to_include = 1:20,
sampled_infections = data$sampled_infections)
# This function takes a list of temporal matrices and returns an intralayer and
# interlayer edge lists in a format: [layer_source, node_source, layer_target node_target, weight].
# It also returns the list of node names
build_infomap_objects <- function(network_object, write_to_infomap_file=T, return_objects=T){
temporal_network <- network_object$temporal_network
base_name <- network_object$base_name
require(igraph)
# Get the node list
nodeLabel <- sort(unique(unlist(lapply(temporal_network,rownames))))
nodeList <- data.frame(nodeID=1:length(nodeLabel), nodeLabel)
# Create intralayer edge lists
layers <- 1:length(temporal_network)
infomap_intralayer <- lapply(layers, function (x) matrix_to_infomap(x, nodeList = nodeList, network_object = network_object))
infomap_intralayer <- do.call("rbind", infomap_intralayer)
# Create interlayer edge lists. Only connect consecutive layers (interlayer edges ONLY go from layer l to l+1).
layers <- layers[-length(layers)]
infomap_interlayer <- lapply(layers, function (x) build_interlayer_edges_1step(x, nodeList = nodeList, network_object = network_object))
infomap_interlayer <- do.call("rbind", infomap_interlayer)
if (write_to_infomap_file){
## Write file for infomap
print('Writing Infomap files')
file <- paste(base_name,'_Infomap_multilayer','.txt',sep='')
print(paste('Infomap file:',file))
if (file.exists(file)){unlink(file)}
sink(file, append = T)
cat("# A network in a general multiplex format");cat('\n')
cat(paste("*Vertices",nrow(nodeList)));cat('\n')
write.table(nodeList, file, append = T,sep=' ', quote = T, row.names = F, col.names = F)
cat("*Multiplex");cat('\n')
cat("# layer node layer node [weight]");cat('\n')
cat("# Intralayer edges");cat('\n')
write.table(infomap_intralayer, file, sep = ' ', row.names = F, col.names = F, quote = F, append = T)
cat("# Interlayer edges");cat('\n')
write.table(infomap_interlayer, file, sep = ' ', row.names = F, col.names = F, quote = F, append = T)
sink.reset()
}
if (return_objects){
return(list(infomap_intralayer=infomap_intralayer, infomap_interlayer=infomap_interlayer, nodeList=nodeList))
}
}
infomap <- build_infomap_objects(network_object = network, write_to_infomap_file = T, return_objects = T)
infomap$nodeList
head(infomap$nodeList)
infomap$infomap_intralayer
tail(infomap$infomap_intralayer)
infomap$infomap_interlayer
system('rm *.sbatch')
scenario_range <- c('G')
exp_range <- sprintf('%0.3d', 0:4)
ps_range <- sprintf('%0.2d', 27:39)
for (ps in ps_range){
for (scenario in scenario_range){
unlink('files_tmp.txt')
sink('files_tmp.txt', append = T)
# Add py files
files <- list.files(path = '~/Documents/malaria_interventions_data/', pattern = '.py', full.names = F)
files <- files[str_sub(files,3,4) %in% ps]
files <- files[str_sub(files,6,6) %in% scenario]
files <- files[str_sub(files,9,11) %in% exp_range]
for (i in 1:length(files)){
cat(files[i]);cat('\n')
}
sink.reset()
# Create zip
# unlink(paste(ps,'_sbatch_py.zip',sep=''))
if (file.size('files_tmp.txt')>10){
system(paste('zip ',ps,'_',scenario,'_py.zip -@ < files_tmp.txt -mu',sep=''))
}
}
}
setwd('~/Documents/malaria_interventions')
source('functions.R')
system('rm *.sbatch')
scenario_range <- c('G')
exp_range <- sprintf('%0.3d', 0:4)
ps_range <- sprintf('%0.2d', 27:39)
for (ps in ps_range){
for (scenario in scenario_range){
unlink('files_tmp.txt')
sink('files_tmp.txt', append = T)
# Add py files
files <- list.files(path = '~/Documents/malaria_interventions_data/', pattern = '.py', full.names = F)
files <- files[str_sub(files,3,4) %in% ps]
files <- files[str_sub(files,6,6) %in% scenario]
files <- files[str_sub(files,9,11) %in% exp_range]
for (i in 1:length(files)){
cat(files[i]);cat('\n')
}
sink.reset()
# Create zip
# unlink(paste(ps,'_sbatch_py.zip',sep=''))
if (file.size('files_tmp.txt')>10){
system(paste('zip ',ps,'_',scenario,'_py.zip -@ < files_tmp.txt -mu',sep=''))
}
}
}
setwd('~/Documents/malaria_interventions_data/')
system('rm *.sbatch')
scenario_range <- c('G')
exp_range <- sprintf('%0.3d', 0:4)
ps_range <- sprintf('%0.2d', 27:39)
for (ps in ps_range){
for (scenario in scenario_range){
unlink('files_tmp.txt')
sink('files_tmp.txt', append = T)
# Add py files
files <- list.files(path = '~/Documents/malaria_interventions_data/', pattern = '.py', full.names = F)
files <- files[str_sub(files,3,4) %in% ps]
files <- files[str_sub(files,6,6) %in% scenario]
files <- files[str_sub(files,9,11) %in% exp_range]
for (i in 1:length(files)){
cat(files[i]);cat('\n')
}
sink.reset()
# Create zip
# unlink(paste(ps,'_sbatch_py.zip',sep=''))
if (file.size('files_tmp.txt')>10){
system(paste('zip ',ps,'_',scenario,'_py.zip -@ < files_tmp.txt -mu',sep=''))
}
}
}
library(tidyverse)
library(magrittr)
library(sqldf)
library(igraph)
library(data.table)
library(googlesheets)
# Functions ---------------------------------------------------------------
## @knitr FUNCTIONS
source('~/Documents/malaria_interventions/functions.R')
## @knitr END
# Initialize important variables ------------------------------------------
setwd('~/Documents/malaria_interventions_data/')
design <- loadExperiments_GoogleSheets()
ps_range <- sprintf('%0.2d', 27:39)
exp_range <- sprintf('%0.3d', 1:4)
run_range <- 1:50
scenario <- 'S'
monitored_variables <- c('prevalence', 'meanMOI','n_circulating_strains', 'n_circulating_genes', 'n_alleles', 'n_total_bites')
exp_cols <- c('black','#0A97B7','#B70A97','#97B70A')
scenario_cols <- c('red','blue','orange')
setwd('~/Documents/malaria_interventions_data/')
ps <- '36'
scenario <- 'S'
exp <- '003'
run <- 1
ps <- '36'
scenario <- 'S'
exp <- '003'
run <- 1
x <- get_data(parameter_space = ps, scenario, experiment = exp, run, use_sqlite = F, tables_to_get = 'summary_general')
x$layer <- group_indices(x, time)
x
group_indices(x, time)
x
x <- get_data(parameter_space = ps, scenario, experiment = exp, run, use_sqlite = F, tables_to_get = 'summary_general')[[1]]
x$layer <- group_indices(x, time)
layers_to_include = 1:max(x$layer)
x %>%
filter(time>28000 & time<35000) %>%
ggplot(aes(x=layer, y=n_circulating_strains))+
geom_line(color='#900C3F', size=1)+
labs(x='Time',y='Diversity')+
scale_x_continuous(breaks = seq(0,max(x$layer),5))+
mytheme
layers_to_include
layers_to_include = 1:300
x %>%
filter(time>28000 & time<35000) %>%
ggplot(aes(x=layer, y=n_circulating_strains))+
geom_line(color='#900C3F', size=1)+
labs(x='Time',y='Diversity')+
scale_x_continuous(breaks = seq(0,max(x$layer),5))+
mytheme
layers_to_include
network_control <- createTemporalNetwork(ps, scenario, exp = '001', run, layers_to_include = layers_to_include, sampled_infections = NULL, cutoff_value = NULL, sparse = F)
layers_to_include = 1:125
x %>%
filter(time>28000 & time<35000) %>%
ggplot(aes(x=layer, y=n_circulating_strains))+
geom_line(color='#900C3F', size=1)+
labs(x='Time',y='Diversity')+
scale_x_continuous(breaks = seq(0,max(x$layer),5))+
mytheme
network_control <- createTemporalNetwork(ps, scenario, exp = '001', run, layers_to_include = layers_to_include, sampled_infections = NULL, cutoff_value = NULL, sparse = F)
setwd('/home/shai/Documents/malaria_interventions_data/tmp')
list.files()
files <- list.files()
f=files[1]
str_locate_all(f, '_' )
f
loc <- str_locate_all(f, '_' )
loc <- loc[,nrow(loc)]
loc
loc <- str_locate_all(f, '_' )[[1]]
loc <- loc[,nrow(loc)]
loc
loc <- str_locate_all(f, '_' )[[1]]
loc <- loc[nrow(loc),ncol(loc)]
loc
str_sub(f, loc,loc) <- '_S_'
f
for (f in files){
loc <- str_locate_all(f, '_' )[[1]]
loc <- loc[nrow(loc),ncol(loc)]
print(f)
file.rename(f, str_sub(f, loc,loc) <- '_S_')
}
13*50*4*(36000/2000)
13*50*4*(10000/2000)
13*50*4*(36000/2000)
4*3*90
50*13*4
library(tidyverse)
library(magrittr)
library(sqldf)
library(rPython)
library(googlesheets)
source('~/Documents/malaria_interventions/functions.R')
setwd('/home/shai/Documents/malaria_interventions')
sqlite_path_global <- '/media/Data/PLOS_Biol/sqlite_S'
parameter_files_path_global <- '/media/Data/PLOS_Biol/parameter_files'
# Clear previous files if necessary
# clear_previous_files(run = 6, exclude_sqlite = F, exclude_CP = F, exclude_control = F, test = T)
# Get data design
# design <- loadExperiments_GoogleSheets(local = T, workBookName = 'PLOS_Biol_design.csv')
design <- loadExperiments_GoogleSheets(local = F, workBookName = 'PLOS_Biol_design', sheetID = 2)
cutoff_design <- expand.grid(PS=sprintf('%0.2d', 4:6),
scenario=c('S','N','G'),
array='1',
cutoff_prob=seq(0.2,0.95,0.05),
stringsAsFactors = F)
cutoff_design$mem_per_cpu <- 32000
cutoff_design$time <- '36:00:00'
cutoff_design$job_name <- paste(cutoff_design$PS, cutoff_design$scenario,'_',cutoff_design$cutoff_prob,sep='')
cutoff_design$job_name <- gsub('\\.','',cutoff_design$job_name)
cutoff_design
i=1
x <- readLines('~/Documents/malaria_interventions/get_data_midway_plosbiol.sbatch')
ps <- cutoff_design$PS[i]
scenario <- cutoff_design$scenario[i]
cutoff_prob <- cutoff_design$cutoff_prob[i]
x
paste('#SBATCH --output=slurm_output/',cutoff_design$job_name[i],'.out',sep='')
paste('#SBATCH --output=slurm_output/',cutoff_design$job_name[i],'_%A_%a.out',sep='')
